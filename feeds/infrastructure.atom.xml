<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>My Thoughts... - Infrastructure</title><link href="https://blog.palladino.me/" rel="alternate"></link><link href="https://blog.palladino.me/feeds/infrastructure.atom.xml" rel="self"></link><id>https://blog.palladino.me/</id><updated>2025-05-31T00:00:00-06:00</updated><entry><title>How to Build a High-Performance Kubernetes Ingress for 1M+ RPS</title><link href="https://blog.palladino.me/kubernetes-ingress-scaling.html" rel="alternate"></link><published>2025-05-31T00:00:00-06:00</published><updated>2025-05-31T00:00:00-06:00</updated><author><name>Jeff Palladino</name></author><id>tag:blog.palladino.me,2025-05-31:/kubernetes-ingress-scaling.html</id><summary type="html">&lt;p&gt;Lessons from building a high-throughput HAProxy-based ingress layer for Kubernetes clusters handling millions of requests per second.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Handling millions of requests per second (RPS) through a Kubernetes cluster is not just a matter of adding replicas—it demands deliberate optimization of ingress design, connection handling, autoscaling, and network I/O. This post distills key strategies we used to scale an HAProxy-based ingress to consistently handle 1M+ RPS on Azure Kubernetes Service (AKS).&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Ingress Stack Overview&lt;/h2&gt;
&lt;p&gt;We used the following stack:
- &lt;strong&gt;HAProxy&lt;/strong&gt; (with custom config map)
- &lt;strong&gt;Azure Kubernetes Service (AKS)&lt;/strong&gt;
- &lt;strong&gt;Horizontal Pod Autoscaler (HPA)&lt;/strong&gt;
- &lt;strong&gt;Node pools tuned for low-latency&lt;/strong&gt;
- &lt;strong&gt;Prometheus + Grafana&lt;/strong&gt; for observability&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;HAProxy Configuration Essentials&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;ConfigMap&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;maxconn-global&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;100000&amp;quot;&lt;/span&gt;
&lt;span class="nt"&gt;maxconn-server&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;10000&amp;quot;&lt;/span&gt;
&lt;span class="nt"&gt;nbthread&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;4&amp;quot;&lt;/span&gt;
&lt;span class="nt"&gt;timeout-client&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;50s&lt;/span&gt;
&lt;span class="nt"&gt;timeout-connect&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;5s&lt;/span&gt;
&lt;span class="nt"&gt;timeout-queue&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;5s&lt;/span&gt;
&lt;span class="nt"&gt;timeout-http-request&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;10s&lt;/span&gt;
&lt;span class="nt"&gt;timeout-http-keep-alive&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;1s&lt;/span&gt;
&lt;span class="nt"&gt;ssl-options&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;no-sslv3 no-tls-tickets no-tlsv10 no-tlsv11&lt;/span&gt;
&lt;span class="nt"&gt;ssl-ciphers&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;These settings ensure fast connection handling, low latency, and strict SSL policies.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Scaling Strategy&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Use &lt;strong&gt;Dedicated Node Pools&lt;/strong&gt; for ingress controllers (separate from app workloads)&lt;/li&gt;
&lt;li&gt;Set &lt;strong&gt;PodDisruptionBudgets&lt;/strong&gt; to avoid draining ingress under load&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;&lt;code&gt;topologySpreadConstraints&lt;/code&gt;&lt;/strong&gt; or &lt;code&gt;podAntiAffinity&lt;/code&gt; to prevent all ingress pods landing on one node&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;HPA Tweaks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Custom metric: &lt;code&gt;sum(rate(requests[1m]))&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Stabilization window: 30s&lt;/li&gt;
&lt;li&gt;Cooldown: 60s&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ensure metrics server and Prometheus Adapter are tuned to avoid lag in metrics reporting.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Connection and Network Limits&lt;/h2&gt;
&lt;p&gt;AKS nodes have system limits:
- &lt;strong&gt;conntrack table&lt;/strong&gt;: Default is ~131072. You'll need to tune this with &lt;code&gt;sysctl&lt;/code&gt; or use node images with extended limits
- &lt;strong&gt;NIC throughput&lt;/strong&gt;: Scale with Standard_Dv4/Dv5 node series
- Watch out for &lt;code&gt;ConntrackFull&lt;/code&gt; and &lt;code&gt;ReadOnlyFilesystem&lt;/code&gt; errors on nodes under stress&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Observability&lt;/h2&gt;
&lt;p&gt;Key metrics to monitor:
- RPS per pod
- Latency P95/P99
- Dropped connections
- conntrack usage&lt;/p&gt;
&lt;p&gt;Recommended tools:
- &lt;strong&gt;Prometheus&lt;/strong&gt;: with &lt;code&gt;haproxy_exporter&lt;/code&gt;
- &lt;strong&gt;Grafana&lt;/strong&gt;: custom dashboards with alerts
- &lt;strong&gt;Kubernetes Events&lt;/strong&gt;: monitor for pod eviction or failed scheduling&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Bonus: Simulate Load Without Overcommit&lt;/h2&gt;
&lt;p&gt;Use &lt;code&gt;wrk&lt;/code&gt;, &lt;code&gt;vegeta&lt;/code&gt;, or &lt;code&gt;k6&lt;/code&gt; to simulate realistic traffic:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;wrk&lt;span class="w"&gt; &lt;/span&gt;-t12&lt;span class="w"&gt; &lt;/span&gt;-c1000&lt;span class="w"&gt; &lt;/span&gt;-d60s&lt;span class="w"&gt; &lt;/span&gt;--latency&lt;span class="w"&gt; &lt;/span&gt;https://your-ingress.example.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This helps avoid triggering false autoscaler signals while still stressing the ingress layer.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Building a high-throughput ingress isn’t just about more pods—it’s about smarter topology, system-level tuning, and proactive observability. With the right HAProxy configuration and node awareness, Kubernetes ingress can scale to serve millions of requests per second reliably.&lt;/p&gt;
&lt;p&gt;Let me know if you'd like a Helm chart, Terraform config, or Azure-specific node tuning guide to go with this.&lt;/p&gt;</content><category term="Infrastructure"></category><category term="kubernetes"></category><category term="ingress"></category><category term="haproxy"></category><category term="scaling"></category><category term="aks"></category><category term="high-performance"></category></entry></feed>