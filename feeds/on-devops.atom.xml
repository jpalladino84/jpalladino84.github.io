<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>My Thoughts... - On DevOps</title><link href="https://blog.palladino.me/" rel="alternate"></link><link href="https://blog.palladino.me/feeds/on-devops.atom.xml" rel="self"></link><id>https://blog.palladino.me/</id><updated>2025-05-31T00:00:00-06:00</updated><entry><title>How to Route Traffic Across Azure and Linode Using Equinix ExpressRoute</title><link href="https://blog.palladino.me/azure-linode-expressroute.html" rel="alternate"></link><published>2025-05-31T00:00:00-06:00</published><updated>2025-05-31T00:00:00-06:00</updated><author><name>Jeff Palladino</name></author><id>tag:blog.palladino.me,2025-05-31:/azure-linode-expressroute.html</id><summary type="html">&lt;p&gt;A step-by-step engineering guide to configuring cross-cloud traffic between Azure and Linode using Equinix ExpressRoute and BGP.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Multi-cloud networking is complex, but necessary when you want to optimize cost, performance, or geographic redundancy. In this guide, I‚Äôll walk through how I routed traffic from Azure to Linode through Equinix ExpressRoute, including the challenges, missteps, and lessons learned.&lt;/p&gt;
&lt;p&gt;This setup enables low-latency, private, and reliable data transfer between Azure and Linode via Equinix‚Äôs fabric and BGP configuration.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Architecture Overview&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Azure&lt;/strong&gt;: Virtual Network (VNet) with ExpressRoute Gateway&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Equinix&lt;/strong&gt;: Virtual Device and Fabric connection&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linode&lt;/strong&gt;: LKE (Linode Kubernetes Engine) with a private subnet&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Protocol&lt;/strong&gt;: BGP over VLAN&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Key Goals:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Avoid internet hops&lt;/li&gt;
&lt;li&gt;Enable deterministic routing&lt;/li&gt;
&lt;li&gt;Support redundancy&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;Step 1: Provision ExpressRoute in Azure&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Create a &lt;strong&gt;Virtual Network Gateway&lt;/strong&gt; with &lt;code&gt;ExpressRoute&lt;/code&gt; SKU&lt;/li&gt;
&lt;li&gt;Link it to a subnet within your Azure VNet&lt;/li&gt;
&lt;li&gt;Create an &lt;strong&gt;ExpressRoute Circuit&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Choose &lt;strong&gt;Equinix&lt;/strong&gt; as the provider and set the peering location&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;üìå Tip: Make sure you don‚Äôt enable Microsoft peering if you‚Äôre only routing to Linode. Use &lt;strong&gt;Private peering&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Step 2: Create the Equinix Fabric Connection&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Log into the &lt;a href="https://fabric.equinix.com/"&gt;Equinix Fabric Portal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Create a &lt;strong&gt;connection&lt;/strong&gt; between Azure and a virtual device (Cisco CSR or Palo Alto works well)&lt;/li&gt;
&lt;li&gt;Assign VLAN tags to each side:&lt;/li&gt;
&lt;li&gt;A-side: Azure (e.g., 10.10.1.1/30)&lt;/li&gt;
&lt;li&gt;Z-side: Linode (e.g., 10.10.2.1/30)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;üí° Watch for &lt;strong&gt;overlapping subnets&lt;/strong&gt; between Azure and Linode! This caused initial route flaps.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Step 3: Configure Linode for BGP&lt;/h2&gt;
&lt;p&gt;Linode doesn‚Äôt natively support BGP, so you‚Äôll need to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Deploy a router VM (e.g., VyOS or FRRouting)&lt;/li&gt;
&lt;li&gt;Assign it a static IP on your private Linode subnet&lt;/li&gt;
&lt;li&gt;Configure BGP neighbor with the Equinix Z-side&lt;/li&gt;
&lt;li&gt;Advertise Linode CIDRs&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2&gt;Step 4: Exchange Routes with Azure&lt;/h2&gt;
&lt;p&gt;In Azure:
- Use &lt;code&gt;Get-AzExpressRouteCircuit&lt;/code&gt; to confirm peering is active
- Check learned routes via &lt;code&gt;Get-AzRouteTable&lt;/code&gt;
- Ensure route tables are associated with subnets in your VNet&lt;/p&gt;
&lt;p&gt;From Linode:
- Ping test Azure private IPs
- Run &lt;code&gt;traceroute&lt;/code&gt; to confirm Equinix path is taken&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Common Pitfalls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;‚ùå &lt;strong&gt;Missing BGP ASN on one side&lt;/strong&gt; ‚Äî causes peering rejection&lt;/li&gt;
&lt;li&gt;‚ùå &lt;strong&gt;Incorrect VLAN tags&lt;/strong&gt; ‚Äî traffic drops silently&lt;/li&gt;
&lt;li&gt;‚ùå &lt;strong&gt;Unroutable return path&lt;/strong&gt; ‚Äî remember to update &lt;strong&gt;both&lt;/strong&gt; route tables&lt;/li&gt;
&lt;li&gt;‚ùå &lt;strong&gt;UDP/ICMP tests failing&lt;/strong&gt; ‚Äî ExpressRoute doesn‚Äôt forward all protocol types by default&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;Bonus: Test End-to-End Application Traffic&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Deploy a sample app in Linode‚Äôs LKE&lt;/li&gt;
&lt;li&gt;Use Azure Container App or App Gateway to hit the endpoint&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;tcpdump&lt;/code&gt; to confirm traffic path&lt;/li&gt;
&lt;li&gt;Add latency monitoring via Prometheus or Grafana&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Routing traffic between Azure and Linode via Equinix ExpressRoute is absolutely possible‚Äîbut requires surgical attention to BGP, subnets, and physical routing topology.&lt;/p&gt;
&lt;p&gt;Once working, the benefits are huge: consistent performance, lower latency, and private, secure inter-cloud communication.&lt;/p&gt;
&lt;p&gt;Let me know if you‚Äôd like a Terraform version of this configuration or a reusable BGP starter template!&lt;/p&gt;</content><category term="On DevOps"></category><category term="azure"></category><category term="expressroute"></category><category term="linode"></category><category term="equinix"></category><category term="bgp"></category><category term="hybrid cloud"></category></entry><entry><title>How to Build a High-Performance Kubernetes Ingress for 1M+ RPS</title><link href="https://blog.palladino.me/kubernetes-ingress-scaling.html" rel="alternate"></link><published>2025-05-31T00:00:00-06:00</published><updated>2025-05-31T00:00:00-06:00</updated><author><name>Jeff Palladino</name></author><id>tag:blog.palladino.me,2025-05-31:/kubernetes-ingress-scaling.html</id><summary type="html">&lt;p&gt;Lessons from building a high-throughput HAProxy-based ingress layer for Kubernetes clusters handling millions of requests per second.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Handling millions of requests per second (RPS) through a Kubernetes cluster is not just a matter of adding replicas‚Äîit demands deliberate optimization of ingress design, connection handling, autoscaling, and network I/O. This post distills key strategies we used to scale an HAProxy-based ingress to consistently handle 1M+ RPS on Azure Kubernetes Service (AKS).&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Ingress Stack Overview&lt;/h2&gt;
&lt;p&gt;We used the following stack:
- &lt;strong&gt;HAProxy&lt;/strong&gt; (with custom config map)
- &lt;strong&gt;Azure Kubernetes Service (AKS)&lt;/strong&gt;
- &lt;strong&gt;Horizontal Pod Autoscaler (HPA)&lt;/strong&gt;
- &lt;strong&gt;Node pools tuned for low-latency&lt;/strong&gt;
- &lt;strong&gt;Prometheus + Grafana&lt;/strong&gt; for observability&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;HAProxy Configuration Essentials&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;ConfigMap&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;maxconn-global&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;100000&amp;quot;&lt;/span&gt;
&lt;span class="nt"&gt;maxconn-server&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;10000&amp;quot;&lt;/span&gt;
&lt;span class="nt"&gt;nbthread&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;4&amp;quot;&lt;/span&gt;
&lt;span class="nt"&gt;timeout-client&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;50s&lt;/span&gt;
&lt;span class="nt"&gt;timeout-connect&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;5s&lt;/span&gt;
&lt;span class="nt"&gt;timeout-queue&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;5s&lt;/span&gt;
&lt;span class="nt"&gt;timeout-http-request&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;10s&lt;/span&gt;
&lt;span class="nt"&gt;timeout-http-keep-alive&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;1s&lt;/span&gt;
&lt;span class="nt"&gt;ssl-options&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;no-sslv3 no-tls-tickets no-tlsv10 no-tlsv11&lt;/span&gt;
&lt;span class="nt"&gt;ssl-ciphers&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;These settings ensure fast connection handling, low latency, and strict SSL policies.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Scaling Strategy&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Use &lt;strong&gt;Dedicated Node Pools&lt;/strong&gt; for ingress controllers (separate from app workloads)&lt;/li&gt;
&lt;li&gt;Set &lt;strong&gt;PodDisruptionBudgets&lt;/strong&gt; to avoid draining ingress under load&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;&lt;code&gt;topologySpreadConstraints&lt;/code&gt;&lt;/strong&gt; or &lt;code&gt;podAntiAffinity&lt;/code&gt; to prevent all ingress pods landing on one node&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;HPA Tweaks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Custom metric: &lt;code&gt;sum(rate(requests[1m]))&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Stabilization window: 30s&lt;/li&gt;
&lt;li&gt;Cooldown: 60s&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ensure metrics server and Prometheus Adapter are tuned to avoid lag in metrics reporting.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Connection and Network Limits&lt;/h2&gt;
&lt;p&gt;AKS nodes have system limits:
- &lt;strong&gt;conntrack table&lt;/strong&gt;: Default is ~131072. You'll need to tune this with &lt;code&gt;sysctl&lt;/code&gt; or use node images with extended limits
- &lt;strong&gt;NIC throughput&lt;/strong&gt;: Scale with Standard_Dv4/Dv5 node series
- Watch out for &lt;code&gt;ConntrackFull&lt;/code&gt; and &lt;code&gt;ReadOnlyFilesystem&lt;/code&gt; errors on nodes under stress&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Observability&lt;/h2&gt;
&lt;p&gt;Key metrics to monitor:
- RPS per pod
- Latency P95/P99
- Dropped connections
- conntrack usage&lt;/p&gt;
&lt;p&gt;Recommended tools:
- &lt;strong&gt;Prometheus&lt;/strong&gt;: with &lt;code&gt;haproxy_exporter&lt;/code&gt;
- &lt;strong&gt;Grafana&lt;/strong&gt;: custom dashboards with alerts
- &lt;strong&gt;Kubernetes Events&lt;/strong&gt;: monitor for pod eviction or failed scheduling&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Bonus: Simulate Load Without Overcommit&lt;/h2&gt;
&lt;p&gt;Use &lt;code&gt;wrk&lt;/code&gt;, &lt;code&gt;vegeta&lt;/code&gt;, or &lt;code&gt;k6&lt;/code&gt; to simulate realistic traffic:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;wrk&lt;span class="w"&gt; &lt;/span&gt;-t12&lt;span class="w"&gt; &lt;/span&gt;-c1000&lt;span class="w"&gt; &lt;/span&gt;-d60s&lt;span class="w"&gt; &lt;/span&gt;--latency&lt;span class="w"&gt; &lt;/span&gt;https://your-ingress.example.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This helps avoid triggering false autoscaler signals while still stressing the ingress layer.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Building a high-throughput ingress isn‚Äôt just about more pods‚Äîit‚Äôs about smarter topology, system-level tuning, and proactive observability. With the right HAProxy configuration and node awareness, Kubernetes ingress can scale to serve millions of requests per second reliably.&lt;/p&gt;
&lt;p&gt;Let me know if you'd like a Helm chart, Terraform config, or Azure-specific node tuning guide to go with this.&lt;/p&gt;</content><category term="On DevOps"></category><category term="kubernetes"></category><category term="ingress"></category><category term="haproxy"></category><category term="scaling"></category><category term="aks"></category><category term="high-performance"></category></entry></feed>